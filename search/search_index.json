{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Apache Spark com Delta Lake e Apache Iceberg Bem-vindo \u00e0 documenta\u00e7\u00e3o do projeto de Apache Spark com Delta Lake e Apache Iceberg. Vis\u00e3o Geral Este projeto implementa um ambiente PySpark com Jupyter Labs, integrando Delta Lake e Apache Iceberg para demonstrar opera\u00e7\u00f5es de INSERT, UPDATE e DELETE em tabelas de dados. Estrutura do Projeto Datalake-house/ \u251c\u2500\u2500 data/ # Diret\u00f3rio para armazenar dados \u251c\u2500\u2500 docs/ # Documenta\u00e7\u00e3o do projeto (MkDocs) \u251c\u2500\u2500 notebooks/ # Notebooks Jupyter \u2502 \u251c\u2500\u2500 delta_lake_operations.ipynb # Exemplos de opera\u00e7\u00f5es com Delta Lake \u2502 \u251c\u2500\u2500 iceberg_operations.ipynb # Exemplos de opera\u00e7\u00f5es com Apache Iceberg \u2502 \u2514\u2500\u2500 delta_iceberg_comparison.ipynb # Compara\u00e7\u00e3o entre Delta Lake e Apache Iceberg \u251c\u2500\u2500 src/ # C\u00f3digo-fonte do projeto \u2502 \u2514\u2500\u2500 spark_delta_iceberg/ # Pacote principal \u251c\u2500\u2500 tests/ # Testes unit\u00e1rios \u251c\u2500\u2500 .gitignore # Arquivos a serem ignorados pelo Git \u251c\u2500\u2500 mkdocs.yml # Configura\u00e7\u00e3o do MkDocs \u251c\u2500\u2500 pyproject.toml # Configura\u00e7\u00e3o do projeto Python \u2514\u2500\u2500 README.md # Documenta\u00e7\u00e3o principal Tecnologias Utilizadas Apache Spark : Framework de processamento distribu\u00eddo para big data Delta Lake : Camada de armazenamento para data lakes que traz confiabilidade ao Apache Spark Apache Iceberg : Formato de tabela de alto desempenho para conjuntos de dados anal\u00edticos enormes Jupyter Labs : Ambiente interativo para desenvolvimento e an\u00e1lise de dados UV : Gerenciador de pacotes Python utilizado no projeto Requisitos Python 3.8 ou superior UV (gerenciador de pacotes Python) Java 8 ou superior (necess\u00e1rio para o Apache Spark) Navega\u00e7\u00e3o Utilize o menu de navega\u00e7\u00e3o para acessar as p\u00e1ginas espec\u00edficas sobre: Delta Lake : Detalhes sobre Delta Lake e exemplos de opera\u00e7\u00f5es Apache Iceberg : Detalhes sobre Apache Iceberg e exemplos de opera\u00e7\u00f5es Autores Alexandre Destro Zanoni Miguel Rossi Fermo","title":"Home"},{"location":"#apache-spark-com-delta-lake-e-apache-iceberg","text":"Bem-vindo \u00e0 documenta\u00e7\u00e3o do projeto de Apache Spark com Delta Lake e Apache Iceberg.","title":"Apache Spark com Delta Lake e Apache Iceberg"},{"location":"#visao-geral","text":"Este projeto implementa um ambiente PySpark com Jupyter Labs, integrando Delta Lake e Apache Iceberg para demonstrar opera\u00e7\u00f5es de INSERT, UPDATE e DELETE em tabelas de dados.","title":"Vis\u00e3o Geral"},{"location":"#estrutura-do-projeto","text":"Datalake-house/ \u251c\u2500\u2500 data/ # Diret\u00f3rio para armazenar dados \u251c\u2500\u2500 docs/ # Documenta\u00e7\u00e3o do projeto (MkDocs) \u251c\u2500\u2500 notebooks/ # Notebooks Jupyter \u2502 \u251c\u2500\u2500 delta_lake_operations.ipynb # Exemplos de opera\u00e7\u00f5es com Delta Lake \u2502 \u251c\u2500\u2500 iceberg_operations.ipynb # Exemplos de opera\u00e7\u00f5es com Apache Iceberg \u2502 \u2514\u2500\u2500 delta_iceberg_comparison.ipynb # Compara\u00e7\u00e3o entre Delta Lake e Apache Iceberg \u251c\u2500\u2500 src/ # C\u00f3digo-fonte do projeto \u2502 \u2514\u2500\u2500 spark_delta_iceberg/ # Pacote principal \u251c\u2500\u2500 tests/ # Testes unit\u00e1rios \u251c\u2500\u2500 .gitignore # Arquivos a serem ignorados pelo Git \u251c\u2500\u2500 mkdocs.yml # Configura\u00e7\u00e3o do MkDocs \u251c\u2500\u2500 pyproject.toml # Configura\u00e7\u00e3o do projeto Python \u2514\u2500\u2500 README.md # Documenta\u00e7\u00e3o principal","title":"Estrutura do Projeto"},{"location":"#tecnologias-utilizadas","text":"Apache Spark : Framework de processamento distribu\u00eddo para big data Delta Lake : Camada de armazenamento para data lakes que traz confiabilidade ao Apache Spark Apache Iceberg : Formato de tabela de alto desempenho para conjuntos de dados anal\u00edticos enormes Jupyter Labs : Ambiente interativo para desenvolvimento e an\u00e1lise de dados UV : Gerenciador de pacotes Python utilizado no projeto","title":"Tecnologias Utilizadas"},{"location":"#requisitos","text":"Python 3.8 ou superior UV (gerenciador de pacotes Python) Java 8 ou superior (necess\u00e1rio para o Apache Spark)","title":"Requisitos"},{"location":"#navegacao","text":"Utilize o menu de navega\u00e7\u00e3o para acessar as p\u00e1ginas espec\u00edficas sobre: Delta Lake : Detalhes sobre Delta Lake e exemplos de opera\u00e7\u00f5es Apache Iceberg : Detalhes sobre Apache Iceberg e exemplos de opera\u00e7\u00f5es","title":"Navega\u00e7\u00e3o"},{"location":"#autores","text":"Alexandre Destro Zanoni Miguel Rossi Fermo","title":"Autores"},{"location":"apache-iceberg/","text":"Apache Iceberg O Apache Iceberg \u00e9 um formato de tabela de alto desempenho para conjuntos de dados anal\u00edticos enormes. Nesta p\u00e1gina, demonstramos as opera\u00e7\u00f5es b\u00e1sicas de INSERT, UPDATE e DELETE usando Apache Iceberg com Apache Spark. O que \u00e9 Apache Iceberg? Apache Iceberg \u00e9 um formato de tabela aberta para conjuntos de dados anal\u00edticos, projetado para abordar os desafios da gest\u00e3o e consulta de grandes conjuntos de dados. Iceberg traz a confiabilidade e simplicidade das tabelas SQL para big data, enquanto torna poss\u00edvel que motores como Spark, Trino, Flink, Presto, Hive e Impala trabalhem com seguran\u00e7a com as mesmas tabelas, ao mesmo tempo. Principais Caracter\u00edsticas Formato de Tabela Aberta : Iceberg \u00e9 um formato de tabela aberto que permite que diferentes motores de processamento acessem os mesmos dados. Transa\u00e7\u00f5es ACID : Garantem a consist\u00eancia dos dados mesmo em caso de falhas. Time Travel : Permite acessar vers\u00f5es anteriores dos dados para auditoria ou rollback. Evolu\u00e7\u00e3o de Esquema : Permite alterar o esquema dos dados sem afetar os consumidores. Opera\u00e7\u00f5es SQL Expressivas : Suporte a opera\u00e7\u00f5es como UPDATE, DELETE e MERGE, que n\u00e3o s\u00e3o nativas em formatos tradicionais de data lake. Particionamento Oculto : O particionamento \u00e9 gerenciado pelo Iceberg, n\u00e3o pelo usu\u00e1rio, o que simplifica o uso. Configura\u00e7\u00e3o do Ambiente Para trabalhar com Apache Iceberg, voc\u00ea precisa configurar uma sess\u00e3o Spark com suporte ao Iceberg: from pyspark.sql import SparkSession # Criar a sess\u00e3o Spark com suporte ao Apache Iceberg spark = SparkSession . builder \\ . appName ( \"IcebergDemo\" ) \\ . config ( \"spark.sql.extensions\" , \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\" ) \\ . config ( \"spark.sql.catalog.iceberg\" , \"org.apache.iceberg.spark.SparkCatalog\" ) \\ . config ( \"spark.sql.catalog.iceberg.type\" , \"hadoop\" ) \\ . config ( \"spark.sql.catalog.iceberg.warehouse\" , \"iceberg-warehouse\" ) \\ . getOrCreate () Opera\u00e7\u00f5es com Apache Iceberg Opera\u00e7\u00e3o INSERT O Apache Iceberg suporta opera\u00e7\u00f5es de inser\u00e7\u00e3o de dados de forma simples e eficiente. Veja como criar uma tabela Iceberg e inserir dados: # Registrar o DataFrame como uma view tempor\u00e1ria df_vendas . createOrReplaceTempView ( \"temp_view\" ) # Criar uma tabela Iceberg spark . sql ( \"\"\" CREATE TABLE IF NOT EXISTS iceberg.default.vendas_iceberg USING iceberg PARTITIONED BY (estado) AS SELECT * FROM temp_view \"\"\" ) # Inserir novos dados df_novos . writeTo ( \"iceberg.default.vendas_iceberg\" ) . append () Opera\u00e7\u00e3o UPDATE O Apache Iceberg permite atualiza\u00e7\u00f5es de dados usando SQL: # Atualizar o pre\u00e7o do Produto B spark . sql ( \"\"\" UPDATE iceberg.default.vendas_iceberg SET preco = 99.90 WHERE nome = 'Produto B' \"\"\" ) # Atualizar o nome e o pre\u00e7o do Produto E spark . sql ( \"\"\" UPDATE iceberg.default.vendas_iceberg SET nome = 'Produto E Premium', preco = 149.90 WHERE nome = 'Produto E' \"\"\" ) Opera\u00e7\u00e3o DELETE O Apache Iceberg suporta exclus\u00e3o de dados de forma eficiente: # Excluir produtos da categoria Alimentos com pre\u00e7o menor que 30 spark . sql ( \"\"\" DELETE FROM iceberg.default.vendas_iceberg WHERE categoria = 'Alimentos' AND preco < 30 \"\"\" ) Opera\u00e7\u00e3o MERGE O MERGE \u00e9 uma opera\u00e7\u00e3o poderosa que permite atualizar e inserir dados em uma \u00fanica opera\u00e7\u00e3o: # Registrar o DataFrame de origem como uma view tempor\u00e1ria df_update . createOrReplaceTempView ( \"source_data_temp\" ) # Realizar a opera\u00e7\u00e3o MERGE spark . sql ( \"\"\" MERGE INTO iceberg.default.vendas_iceberg target USING source_data_temp source ON target.id = source.id WHEN MATCHED THEN UPDATE SET target.nome = source.nome, target.preco = source.preco WHEN NOT MATCHED THEN INSERT * \"\"\" ) Time Travel O Apache Iceberg permite acessar vers\u00f5es anteriores dos dados: # Obter os snapshots da tabela snapshots_df = spark . sql ( \"SELECT * FROM iceberg.default.vendas_iceberg.snapshots\" ) # Acessar um snapshot espec\u00edfico first_snapshot_id = snapshots_df . select ( \"snapshot_id\" ) . first ()[ 0 ] df_snapshot = spark . read . option ( \"snapshot-id\" , first_snapshot_id ) . table ( \"iceberg.default.vendas_iceberg\" ) # Acessar a tabela em um timestamp espec\u00edfico df_timestamp = spark . read . option ( \"as-of-timestamp\" , \"2023-01-01 00:00:00\" ) . table ( \"iceberg.default.vendas_iceberg\" ) Evolu\u00e7\u00e3o de Esquema O Apache Iceberg suporta evolu\u00e7\u00e3o de esquema de forma transparente: # Adicionar uma nova coluna \u00e0 tabela spark . sql ( \"ALTER TABLE iceberg.default.vendas_iceberg ADD COLUMN desconto DOUBLE\" ) # Atualizar alguns registros com valores para a nova coluna spark . sql ( \"\"\" UPDATE iceberg.default.vendas_iceberg SET desconto = preco * 0.1 WHERE categoria = 'Eletr\u00f4nicos' \"\"\" ) Vantagens do Apache Iceberg Formato de Tabela Aberta : Iceberg \u00e9 um formato de tabela aberto que permite que diferentes motores de processamento acessem os mesmos dados. Transa\u00e7\u00f5es ACID : Garantem a consist\u00eancia dos dados mesmo em caso de falhas. Time Travel : Permite acessar vers\u00f5es anteriores dos dados para auditoria ou rollback. Evolu\u00e7\u00e3o de Esquema : Permite alterar o esquema dos dados sem afetar os consumidores. Opera\u00e7\u00f5es SQL Expressivas : Suporte a opera\u00e7\u00f5es como UPDATE, DELETE e MERGE, que n\u00e3o s\u00e3o nativas em formatos tradicionais de data lake. Particionamento Oculto : O particionamento \u00e9 gerenciado pelo Iceberg, n\u00e3o pelo usu\u00e1rio, o que simplifica o uso. Exemplo Completo Para um exemplo completo de opera\u00e7\u00f5es com Apache Iceberg, consulte o notebook iceberg_operations.ipynb inclu\u00eddo neste projeto. Refer\u00eancias Site oficial do Apache Iceberg Documenta\u00e7\u00e3o do Apache Iceberg GitHub do Apache Iceberg","title":"Apache Iceberg"},{"location":"apache-iceberg/#apache-iceberg","text":"O Apache Iceberg \u00e9 um formato de tabela de alto desempenho para conjuntos de dados anal\u00edticos enormes. Nesta p\u00e1gina, demonstramos as opera\u00e7\u00f5es b\u00e1sicas de INSERT, UPDATE e DELETE usando Apache Iceberg com Apache Spark.","title":"Apache Iceberg"},{"location":"apache-iceberg/#o-que-e-apache-iceberg","text":"Apache Iceberg \u00e9 um formato de tabela aberta para conjuntos de dados anal\u00edticos, projetado para abordar os desafios da gest\u00e3o e consulta de grandes conjuntos de dados. Iceberg traz a confiabilidade e simplicidade das tabelas SQL para big data, enquanto torna poss\u00edvel que motores como Spark, Trino, Flink, Presto, Hive e Impala trabalhem com seguran\u00e7a com as mesmas tabelas, ao mesmo tempo.","title":"O que \u00e9 Apache Iceberg?"},{"location":"apache-iceberg/#principais-caracteristicas","text":"Formato de Tabela Aberta : Iceberg \u00e9 um formato de tabela aberto que permite que diferentes motores de processamento acessem os mesmos dados. Transa\u00e7\u00f5es ACID : Garantem a consist\u00eancia dos dados mesmo em caso de falhas. Time Travel : Permite acessar vers\u00f5es anteriores dos dados para auditoria ou rollback. Evolu\u00e7\u00e3o de Esquema : Permite alterar o esquema dos dados sem afetar os consumidores. Opera\u00e7\u00f5es SQL Expressivas : Suporte a opera\u00e7\u00f5es como UPDATE, DELETE e MERGE, que n\u00e3o s\u00e3o nativas em formatos tradicionais de data lake. Particionamento Oculto : O particionamento \u00e9 gerenciado pelo Iceberg, n\u00e3o pelo usu\u00e1rio, o que simplifica o uso.","title":"Principais Caracter\u00edsticas"},{"location":"apache-iceberg/#configuracao-do-ambiente","text":"Para trabalhar com Apache Iceberg, voc\u00ea precisa configurar uma sess\u00e3o Spark com suporte ao Iceberg: from pyspark.sql import SparkSession # Criar a sess\u00e3o Spark com suporte ao Apache Iceberg spark = SparkSession . builder \\ . appName ( \"IcebergDemo\" ) \\ . config ( \"spark.sql.extensions\" , \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\" ) \\ . config ( \"spark.sql.catalog.iceberg\" , \"org.apache.iceberg.spark.SparkCatalog\" ) \\ . config ( \"spark.sql.catalog.iceberg.type\" , \"hadoop\" ) \\ . config ( \"spark.sql.catalog.iceberg.warehouse\" , \"iceberg-warehouse\" ) \\ . getOrCreate ()","title":"Configura\u00e7\u00e3o do Ambiente"},{"location":"apache-iceberg/#operacoes-com-apache-iceberg","text":"","title":"Opera\u00e7\u00f5es com Apache Iceberg"},{"location":"apache-iceberg/#operacao-insert","text":"O Apache Iceberg suporta opera\u00e7\u00f5es de inser\u00e7\u00e3o de dados de forma simples e eficiente. Veja como criar uma tabela Iceberg e inserir dados: # Registrar o DataFrame como uma view tempor\u00e1ria df_vendas . createOrReplaceTempView ( \"temp_view\" ) # Criar uma tabela Iceberg spark . sql ( \"\"\" CREATE TABLE IF NOT EXISTS iceberg.default.vendas_iceberg USING iceberg PARTITIONED BY (estado) AS SELECT * FROM temp_view \"\"\" ) # Inserir novos dados df_novos . writeTo ( \"iceberg.default.vendas_iceberg\" ) . append ()","title":"Opera\u00e7\u00e3o INSERT"},{"location":"apache-iceberg/#operacao-update","text":"O Apache Iceberg permite atualiza\u00e7\u00f5es de dados usando SQL: # Atualizar o pre\u00e7o do Produto B spark . sql ( \"\"\" UPDATE iceberg.default.vendas_iceberg SET preco = 99.90 WHERE nome = 'Produto B' \"\"\" ) # Atualizar o nome e o pre\u00e7o do Produto E spark . sql ( \"\"\" UPDATE iceberg.default.vendas_iceberg SET nome = 'Produto E Premium', preco = 149.90 WHERE nome = 'Produto E' \"\"\" )","title":"Opera\u00e7\u00e3o UPDATE"},{"location":"apache-iceberg/#operacao-delete","text":"O Apache Iceberg suporta exclus\u00e3o de dados de forma eficiente: # Excluir produtos da categoria Alimentos com pre\u00e7o menor que 30 spark . sql ( \"\"\" DELETE FROM iceberg.default.vendas_iceberg WHERE categoria = 'Alimentos' AND preco < 30 \"\"\" )","title":"Opera\u00e7\u00e3o DELETE"},{"location":"apache-iceberg/#operacao-merge","text":"O MERGE \u00e9 uma opera\u00e7\u00e3o poderosa que permite atualizar e inserir dados em uma \u00fanica opera\u00e7\u00e3o: # Registrar o DataFrame de origem como uma view tempor\u00e1ria df_update . createOrReplaceTempView ( \"source_data_temp\" ) # Realizar a opera\u00e7\u00e3o MERGE spark . sql ( \"\"\" MERGE INTO iceberg.default.vendas_iceberg target USING source_data_temp source ON target.id = source.id WHEN MATCHED THEN UPDATE SET target.nome = source.nome, target.preco = source.preco WHEN NOT MATCHED THEN INSERT * \"\"\" )","title":"Opera\u00e7\u00e3o MERGE"},{"location":"apache-iceberg/#time-travel","text":"O Apache Iceberg permite acessar vers\u00f5es anteriores dos dados: # Obter os snapshots da tabela snapshots_df = spark . sql ( \"SELECT * FROM iceberg.default.vendas_iceberg.snapshots\" ) # Acessar um snapshot espec\u00edfico first_snapshot_id = snapshots_df . select ( \"snapshot_id\" ) . first ()[ 0 ] df_snapshot = spark . read . option ( \"snapshot-id\" , first_snapshot_id ) . table ( \"iceberg.default.vendas_iceberg\" ) # Acessar a tabela em um timestamp espec\u00edfico df_timestamp = spark . read . option ( \"as-of-timestamp\" , \"2023-01-01 00:00:00\" ) . table ( \"iceberg.default.vendas_iceberg\" )","title":"Time Travel"},{"location":"apache-iceberg/#evolucao-de-esquema","text":"O Apache Iceberg suporta evolu\u00e7\u00e3o de esquema de forma transparente: # Adicionar uma nova coluna \u00e0 tabela spark . sql ( \"ALTER TABLE iceberg.default.vendas_iceberg ADD COLUMN desconto DOUBLE\" ) # Atualizar alguns registros com valores para a nova coluna spark . sql ( \"\"\" UPDATE iceberg.default.vendas_iceberg SET desconto = preco * 0.1 WHERE categoria = 'Eletr\u00f4nicos' \"\"\" )","title":"Evolu\u00e7\u00e3o de Esquema"},{"location":"apache-iceberg/#vantagens-do-apache-iceberg","text":"Formato de Tabela Aberta : Iceberg \u00e9 um formato de tabela aberto que permite que diferentes motores de processamento acessem os mesmos dados. Transa\u00e7\u00f5es ACID : Garantem a consist\u00eancia dos dados mesmo em caso de falhas. Time Travel : Permite acessar vers\u00f5es anteriores dos dados para auditoria ou rollback. Evolu\u00e7\u00e3o de Esquema : Permite alterar o esquema dos dados sem afetar os consumidores. Opera\u00e7\u00f5es SQL Expressivas : Suporte a opera\u00e7\u00f5es como UPDATE, DELETE e MERGE, que n\u00e3o s\u00e3o nativas em formatos tradicionais de data lake. Particionamento Oculto : O particionamento \u00e9 gerenciado pelo Iceberg, n\u00e3o pelo usu\u00e1rio, o que simplifica o uso.","title":"Vantagens do Apache Iceberg"},{"location":"apache-iceberg/#exemplo-completo","text":"Para um exemplo completo de opera\u00e7\u00f5es com Apache Iceberg, consulte o notebook iceberg_operations.ipynb inclu\u00eddo neste projeto.","title":"Exemplo Completo"},{"location":"apache-iceberg/#referencias","text":"Site oficial do Apache Iceberg Documenta\u00e7\u00e3o do Apache Iceberg GitHub do Apache Iceberg","title":"Refer\u00eancias"},{"location":"delta-lake/","text":"Delta Lake O Delta Lake \u00e9 uma camada de armazenamento de c\u00f3digo aberto que adiciona confiabilidade ao seu data lake. Nesta p\u00e1gina, demonstramos as opera\u00e7\u00f5es b\u00e1sicas de INSERT, UPDATE e DELETE usando Delta Lake com Apache Spark. O que \u00e9 Delta Lake? Delta Lake \u00e9 um framework de armazenamento de c\u00f3digo aberto que permite a constru\u00e7\u00e3o de uma arquitetura lakehouse agn\u00f3stica de formato. \u00c9 uma camada de armazenamento otimizada que proporciona a base para as tabelas em uma inst\u00e2ncia de data lake. Principais Caracter\u00edsticas Transa\u00e7\u00f5es ACID : Protege seus dados com serializabilidade, o n\u00edvel mais forte de isolamento Metadados Escal\u00e1veis : Lida com tabelas de escala petabyte com bilh\u00f5es de parti\u00e7\u00f5es e arquivos com facilidade Time Travel : Acessa/reverte para vers\u00f5es anteriores de dados para auditorias, rollbacks ou reprodu\u00e7\u00e3o C\u00f3digo Aberto : Comunidade dirigida, padr\u00f5es abertos, protocolo aberto, discuss\u00f5es abertas Batch/Streaming Unificado : Sem\u00e2ntica de ingest\u00e3o exatamente uma vez para backfill e consultas interativas Evolu\u00e7\u00e3o/Aplica\u00e7\u00e3o de Esquema : Previne que dados ruins causem corrup\u00e7\u00e3o de dados Hist\u00f3rico de Auditoria : Delta Lake registra todos os detalhes de altera\u00e7\u00f5es, fornecendo um hist\u00f3rico completo de auditoria Opera\u00e7\u00f5es DML : APIs SQL, Scala/Java e Python para mesclar, atualizar e excluir conjuntos de dados Configura\u00e7\u00e3o do Ambiente Para trabalhar com Delta Lake, voc\u00ea precisa configurar uma sess\u00e3o Spark com suporte ao Delta Lake: from pyspark.sql import SparkSession # Criar a sess\u00e3o Spark com suporte ao Delta Lake spark = SparkSession . builder \\ . appName ( \"DeltaLakeDemo\" ) \\ . config ( \"spark.sql.extensions\" , \"io.delta.sql.DeltaSparkSessionExtension\" ) \\ . config ( \"spark.sql.catalog.spark_catalog\" , \"org.apache.spark.sql.delta.catalog.DeltaCatalog\" ) \\ . config ( \"spark.databricks.delta.retentionDurationCheck.enabled\" , \"false\" ) \\ . config ( \"spark.databricks.delta.schema.autoMerge.enabled\" , \"true\" ) \\ . getOrCreate () Opera\u00e7\u00f5es com Delta Lake Opera\u00e7\u00e3o INSERT O Delta Lake suporta opera\u00e7\u00f5es de inser\u00e7\u00e3o de dados de forma simples e eficiente. Veja como criar uma tabela Delta e inserir dados: # Criar uma tabela Delta df_vendas . write \\ . format ( \"delta\" ) \\ . mode ( \"overwrite\" ) \\ . partitionBy ( \"estado\" ) \\ . save ( \"/caminho/para/tabela/delta\" ) # Inserir novos dados df_novos . write \\ . format ( \"delta\" ) \\ . mode ( \"append\" ) \\ . save ( \"/caminho/para/tabela/delta\" ) Opera\u00e7\u00e3o UPDATE O Delta Lake permite atualiza\u00e7\u00f5es de dados usando SQL ou a API do Delta: # Usando SQL spark . sql ( \"\"\" UPDATE delta.`/caminho/para/tabela/delta` SET preco = 99.90 WHERE nome = 'Produto B' \"\"\" ) # Usando a API do Delta from delta.tables import DeltaTable deltaTable = DeltaTable . forPath ( spark , \"/caminho/para/tabela/delta\" ) deltaTable . update ( condition = \"nome = 'Produto E'\" , set = { \"nome\" : \"'Produto E Premium'\" , \"preco\" : \"149.90\" } ) Opera\u00e7\u00e3o DELETE O Delta Lake suporta exclus\u00e3o de dados de forma eficiente: # Usando SQL spark . sql ( \"\"\" DELETE FROM delta.`/caminho/para/tabela/delta` WHERE categoria = 'Alimentos' AND preco < 30 \"\"\" ) # Usando a API do Delta deltaTable = DeltaTable . forPath ( spark , \"/caminho/para/tabela/delta\" ) deltaTable . delete ( \"categoria = 'Alimentos' AND preco < 30\" ) Opera\u00e7\u00e3o MERGE O MERGE \u00e9 uma opera\u00e7\u00e3o poderosa que permite atualizar e inserir dados em uma \u00fanica opera\u00e7\u00e3o: # Registrar o DataFrame de origem como uma view tempor\u00e1ria df_update . createOrReplaceTempView ( \"source_data_temp\" ) # Realizar a opera\u00e7\u00e3o MERGE spark . sql ( \"\"\" MERGE INTO delta.`/caminho/para/tabela/delta` target USING source_data_temp source ON target.id = source.id WHEN MATCHED THEN UPDATE SET target.nome = source.nome, target.preco = source.preco WHEN NOT MATCHED THEN INSERT * \"\"\" ) Time Travel O Delta Lake permite acessar vers\u00f5es anteriores dos dados: # Obter o hist\u00f3rico da tabela history_df = spark . sql ( \"DESCRIBE HISTORY delta.`/caminho/para/tabela/delta`\" ) # Acessar a vers\u00e3o 0 (inicial) da tabela df_versao_0 = spark . read . format ( \"delta\" ) . option ( \"versionAsOf\" , 0 ) . load ( \"/caminho/para/tabela/delta\" ) # Acessar a tabela em um timestamp espec\u00edfico df_timestamp = spark . read . format ( \"delta\" ) . option ( \"timestampAsOf\" , \"2023-01-01 00:00:00\" ) . load ( \"/caminho/para/tabela/delta\" ) Vantagens do Delta Lake Transa\u00e7\u00f5es ACID : Garantem a consist\u00eancia dos dados mesmo em caso de falhas. Time Travel : Permite acessar vers\u00f5es anteriores dos dados para auditoria ou rollback. Opera\u00e7\u00f5es DML : Suporte a opera\u00e7\u00f5es como UPDATE, DELETE e MERGE, que n\u00e3o s\u00e3o nativas em formatos tradicionais de data lake. Evolu\u00e7\u00e3o de Esquema : Permite alterar o esquema dos dados sem afetar os consumidores. Unifica\u00e7\u00e3o de Batch e Streaming : Permite processar dados em batch e streaming de forma unificada. Exemplo Completo Para um exemplo completo de opera\u00e7\u00f5es com Delta Lake, consulte o notebook delta_lake_operations.ipynb inclu\u00eddo neste projeto. Refer\u00eancias Site oficial do Delta Lake Documenta\u00e7\u00e3o do Delta Lake GitHub do Delta Lake","title":"Delta Lake"},{"location":"delta-lake/#delta-lake","text":"O Delta Lake \u00e9 uma camada de armazenamento de c\u00f3digo aberto que adiciona confiabilidade ao seu data lake. Nesta p\u00e1gina, demonstramos as opera\u00e7\u00f5es b\u00e1sicas de INSERT, UPDATE e DELETE usando Delta Lake com Apache Spark.","title":"Delta Lake"},{"location":"delta-lake/#o-que-e-delta-lake","text":"Delta Lake \u00e9 um framework de armazenamento de c\u00f3digo aberto que permite a constru\u00e7\u00e3o de uma arquitetura lakehouse agn\u00f3stica de formato. \u00c9 uma camada de armazenamento otimizada que proporciona a base para as tabelas em uma inst\u00e2ncia de data lake.","title":"O que \u00e9 Delta Lake?"},{"location":"delta-lake/#principais-caracteristicas","text":"Transa\u00e7\u00f5es ACID : Protege seus dados com serializabilidade, o n\u00edvel mais forte de isolamento Metadados Escal\u00e1veis : Lida com tabelas de escala petabyte com bilh\u00f5es de parti\u00e7\u00f5es e arquivos com facilidade Time Travel : Acessa/reverte para vers\u00f5es anteriores de dados para auditorias, rollbacks ou reprodu\u00e7\u00e3o C\u00f3digo Aberto : Comunidade dirigida, padr\u00f5es abertos, protocolo aberto, discuss\u00f5es abertas Batch/Streaming Unificado : Sem\u00e2ntica de ingest\u00e3o exatamente uma vez para backfill e consultas interativas Evolu\u00e7\u00e3o/Aplica\u00e7\u00e3o de Esquema : Previne que dados ruins causem corrup\u00e7\u00e3o de dados Hist\u00f3rico de Auditoria : Delta Lake registra todos os detalhes de altera\u00e7\u00f5es, fornecendo um hist\u00f3rico completo de auditoria Opera\u00e7\u00f5es DML : APIs SQL, Scala/Java e Python para mesclar, atualizar e excluir conjuntos de dados","title":"Principais Caracter\u00edsticas"},{"location":"delta-lake/#configuracao-do-ambiente","text":"Para trabalhar com Delta Lake, voc\u00ea precisa configurar uma sess\u00e3o Spark com suporte ao Delta Lake: from pyspark.sql import SparkSession # Criar a sess\u00e3o Spark com suporte ao Delta Lake spark = SparkSession . builder \\ . appName ( \"DeltaLakeDemo\" ) \\ . config ( \"spark.sql.extensions\" , \"io.delta.sql.DeltaSparkSessionExtension\" ) \\ . config ( \"spark.sql.catalog.spark_catalog\" , \"org.apache.spark.sql.delta.catalog.DeltaCatalog\" ) \\ . config ( \"spark.databricks.delta.retentionDurationCheck.enabled\" , \"false\" ) \\ . config ( \"spark.databricks.delta.schema.autoMerge.enabled\" , \"true\" ) \\ . getOrCreate ()","title":"Configura\u00e7\u00e3o do Ambiente"},{"location":"delta-lake/#operacoes-com-delta-lake","text":"","title":"Opera\u00e7\u00f5es com Delta Lake"},{"location":"delta-lake/#operacao-insert","text":"O Delta Lake suporta opera\u00e7\u00f5es de inser\u00e7\u00e3o de dados de forma simples e eficiente. Veja como criar uma tabela Delta e inserir dados: # Criar uma tabela Delta df_vendas . write \\ . format ( \"delta\" ) \\ . mode ( \"overwrite\" ) \\ . partitionBy ( \"estado\" ) \\ . save ( \"/caminho/para/tabela/delta\" ) # Inserir novos dados df_novos . write \\ . format ( \"delta\" ) \\ . mode ( \"append\" ) \\ . save ( \"/caminho/para/tabela/delta\" )","title":"Opera\u00e7\u00e3o INSERT"},{"location":"delta-lake/#operacao-update","text":"O Delta Lake permite atualiza\u00e7\u00f5es de dados usando SQL ou a API do Delta: # Usando SQL spark . sql ( \"\"\" UPDATE delta.`/caminho/para/tabela/delta` SET preco = 99.90 WHERE nome = 'Produto B' \"\"\" ) # Usando a API do Delta from delta.tables import DeltaTable deltaTable = DeltaTable . forPath ( spark , \"/caminho/para/tabela/delta\" ) deltaTable . update ( condition = \"nome = 'Produto E'\" , set = { \"nome\" : \"'Produto E Premium'\" , \"preco\" : \"149.90\" } )","title":"Opera\u00e7\u00e3o UPDATE"},{"location":"delta-lake/#operacao-delete","text":"O Delta Lake suporta exclus\u00e3o de dados de forma eficiente: # Usando SQL spark . sql ( \"\"\" DELETE FROM delta.`/caminho/para/tabela/delta` WHERE categoria = 'Alimentos' AND preco < 30 \"\"\" ) # Usando a API do Delta deltaTable = DeltaTable . forPath ( spark , \"/caminho/para/tabela/delta\" ) deltaTable . delete ( \"categoria = 'Alimentos' AND preco < 30\" )","title":"Opera\u00e7\u00e3o DELETE"},{"location":"delta-lake/#operacao-merge","text":"O MERGE \u00e9 uma opera\u00e7\u00e3o poderosa que permite atualizar e inserir dados em uma \u00fanica opera\u00e7\u00e3o: # Registrar o DataFrame de origem como uma view tempor\u00e1ria df_update . createOrReplaceTempView ( \"source_data_temp\" ) # Realizar a opera\u00e7\u00e3o MERGE spark . sql ( \"\"\" MERGE INTO delta.`/caminho/para/tabela/delta` target USING source_data_temp source ON target.id = source.id WHEN MATCHED THEN UPDATE SET target.nome = source.nome, target.preco = source.preco WHEN NOT MATCHED THEN INSERT * \"\"\" )","title":"Opera\u00e7\u00e3o MERGE"},{"location":"delta-lake/#time-travel","text":"O Delta Lake permite acessar vers\u00f5es anteriores dos dados: # Obter o hist\u00f3rico da tabela history_df = spark . sql ( \"DESCRIBE HISTORY delta.`/caminho/para/tabela/delta`\" ) # Acessar a vers\u00e3o 0 (inicial) da tabela df_versao_0 = spark . read . format ( \"delta\" ) . option ( \"versionAsOf\" , 0 ) . load ( \"/caminho/para/tabela/delta\" ) # Acessar a tabela em um timestamp espec\u00edfico df_timestamp = spark . read . format ( \"delta\" ) . option ( \"timestampAsOf\" , \"2023-01-01 00:00:00\" ) . load ( \"/caminho/para/tabela/delta\" )","title":"Time Travel"},{"location":"delta-lake/#vantagens-do-delta-lake","text":"Transa\u00e7\u00f5es ACID : Garantem a consist\u00eancia dos dados mesmo em caso de falhas. Time Travel : Permite acessar vers\u00f5es anteriores dos dados para auditoria ou rollback. Opera\u00e7\u00f5es DML : Suporte a opera\u00e7\u00f5es como UPDATE, DELETE e MERGE, que n\u00e3o s\u00e3o nativas em formatos tradicionais de data lake. Evolu\u00e7\u00e3o de Esquema : Permite alterar o esquema dos dados sem afetar os consumidores. Unifica\u00e7\u00e3o de Batch e Streaming : Permite processar dados em batch e streaming de forma unificada.","title":"Vantagens do Delta Lake"},{"location":"delta-lake/#exemplo-completo","text":"Para um exemplo completo de opera\u00e7\u00f5es com Delta Lake, consulte o notebook delta_lake_operations.ipynb inclu\u00eddo neste projeto.","title":"Exemplo Completo"},{"location":"delta-lake/#referencias","text":"Site oficial do Delta Lake Documenta\u00e7\u00e3o do Delta Lake GitHub do Delta Lake","title":"Refer\u00eancias"},{"location":"pesquisa/","text":"Pesquisa sobre Apache Spark, Delta Lake e Apache Iceberg Apache Spark Defini\u00e7\u00e3o e Caracter\u00edsticas O Apache Spark \u00e9 um sistema de processamento distribu\u00eddo de c\u00f3digo aberto usado para workloads de big data. O sistema utiliza armazenamento em cache na mem\u00f3ria e execu\u00e7\u00e3o otimizada de consultas para processar dados de qualquer tamanho de forma eficiente. Hist\u00f3ria O Apache Spark come\u00e7ou em 2009 como um projeto de pesquisa no AMPLab da UC Berkeley, uma colabora\u00e7\u00e3o envolvendo estudantes, pesquisadores e professores, focada em dom\u00ednios de aplica\u00e7\u00f5es com uso intensivo de dados. O primeiro artigo intitulado \"Spark: Cluster Computing with Working Sets\" foi publicado em junho de 2010, e o Spark era de c\u00f3digo aberto sob uma licen\u00e7a BSD. Em junho de 2013, o Spark passou para o status de incuba\u00e7\u00e3o na Apache Software Foundation (ASF) e se estabeleceu como um projeto de alto n\u00edvel da Apache em fevereiro de 2014. Funcionamento O Hadoop MapReduce \u00e9 um modelo de programa\u00e7\u00e3o para processar conjuntos de big data com um algoritmo distribu\u00eddo paralelo, mas tem limita\u00e7\u00f5es devido ao processo sequencial de v\u00e1rias etapas necess\u00e1rio para executar um trabalho. O Spark foi criado para resolver essas limita\u00e7\u00f5es, processando na mem\u00f3ria, reduzindo o n\u00famero de etapas em uma tarefa e reutilizando dados em v\u00e1rias opera\u00e7\u00f5es paralelas. Com o Spark, \u00e9 necess\u00e1ria apenas uma etapa em que os dados s\u00e3o lidos na mem\u00f3ria, as opera\u00e7\u00f5es s\u00e3o executadas e os resultados s\u00e3o gravados de volta, resultando em uma execu\u00e7\u00e3o muito mais r\u00e1pida. O Spark tamb\u00e9m reutiliza dados usando um cache na mem\u00f3ria para acelerar consideravelmente os algoritmos de machine learning que chamam repetidamente uma fun\u00e7\u00e3o no mesmo conjunto de dados. A reutiliza\u00e7\u00e3o de dados \u00e9 realizada por meio da cria\u00e7\u00e3o de DataFrames, uma abstra\u00e7\u00e3o sobre o Conjunto de dados resiliente distribu\u00eddo (RDD), que \u00e9 uma cole\u00e7\u00e3o de objetos armazenados em cache na mem\u00f3ria e reutilizados em v\u00e1rias opera\u00e7\u00f5es do Spark. Isso reduz drasticamente a lat\u00eancia, fazendo com que o Spark seja v\u00e1rias vezes mais r\u00e1pido que o MapReduce, especialmente ao realizar machine learning e an\u00e1lises interativas. Benef\u00edcios R\u00e1pido : Por meio do armazenamento em cache na mem\u00f3ria e execu\u00e7\u00e3o otimizada de consultas, o Spark pode oferecer consultas anal\u00edticas r\u00e1pidas de dados de qualquer tamanho. Para desenvolvedores : O Apache Spark suporta de modo nativo Java, Scala, R e Python, oferecendo v\u00e1rias linguagens para a cria\u00e7\u00e3o de aplicativos. Essas APIs facilitam as coisas para seus desenvolvedores, pois ocultam a complexidade do processamento distribu\u00eddo por tr\u00e1s de operadores simples e de alto n\u00edvel que reduzem drasticamente a quantidade de c\u00f3digo necess\u00e1ria. V\u00e1rias workloads : O Apache Spark vem com a capacidade de executar v\u00e1rias workloads, incluindo consultas interativas, an\u00e1lises em tempo real, machine learning e processamento de gr\u00e1ficos. Uma aplica\u00e7\u00e3o pode combinar v\u00e1rias workloads facilmente. Diferen\u00e7as entre Spark e Hadoop Al\u00e9m das diferen\u00e7as no design do Spark e do Hadoop MapReduce, muitas organiza\u00e7\u00f5es descobriram que essas estruturas de big data s\u00e3o complementares, usando-as juntas para resolver um desafio comercial mais amplo. O Hadoop \u00e9 uma estrutura de c\u00f3digo aberto que tem o Sistema de Arquivos Distribu\u00eddo do Hadoop (HDFS) como armazenamento, o YARN como gerenciamento de recursos e a programa\u00e7\u00e3o MapReduce como mecanismo de execu\u00e7\u00e3o. Em uma implementa\u00e7\u00e3o t\u00edpica do Hadoop, diferentes mecanismos de execu\u00e7\u00e3o tamb\u00e9m s\u00e3o implantados, como Spark, Tez e Presto. O Spark \u00e9 uma estrutura de c\u00f3digo aberto focada em consultas interativas, machine learning e workloads em tempo real. N\u00e3o tem seu pr\u00f3prio sistema de armazenamento, mas executa an\u00e1lises em outros sistemas de armazenamento, como o HDFS, ou em outras lojas populares, como Amazon Redshift, Amazon S3, Couchbase, Cassandra e outras. Delta Lake Defini\u00e7\u00e3o e Caracter\u00edsticas Delta Lake \u00e9 um framework de armazenamento de c\u00f3digo aberto que permite a constru\u00e7\u00e3o de uma arquitetura lakehouse agn\u00f3stica de formato. \u00c9 uma camada de armazenamento otimizada que proporciona a base para as tabelas em uma inst\u00e2ncia de data lake. Funcionalidades Principais Transa\u00e7\u00f5es ACID : Protege seus dados com serializabilidade, o n\u00edvel mais forte de isolamento Metadados Escal\u00e1veis : Lida com tabelas de escala petabyte com bilh\u00f5es de parti\u00e7\u00f5es e arquivos com facilidade Viagem no Tempo (Time Travel) : Acessa/reverte para vers\u00f5es anteriores de dados para auditorias, rollbacks ou reprodu\u00e7\u00e3o C\u00f3digo Aberto : Orientado pela comunidade, padr\u00f5es abertos, protocolo aberto, discuss\u00f5es abertas Batch/Streaming Unificado : Sem\u00e2ntica de ingest\u00e3o exatamente uma vez para backfill e consultas interativas Evolu\u00e7\u00e3o/Aplica\u00e7\u00e3o de Esquema : Previne que dados ruins causem corrup\u00e7\u00e3o de dados Hist\u00f3rico de Auditoria : Delta Lake registra todos os detalhes de altera\u00e7\u00f5es, fornecendo um hist\u00f3rico completo de auditoria Opera\u00e7\u00f5es DML : APIs SQL, Scala/Java e Python para mesclar, atualizar e excluir conjuntos de dados Integra\u00e7\u00e3o com Ecossistemas Delta Lake funciona com motores de computa\u00e7\u00e3o incluindo Spark, PrestoDB, Flink, Trino, Hive, Snowflake, Google BigQuery, Athena, Redshift, Databricks, Azure Fabric e APIs para Scala, Java, Rust e Python. Com o Delta Universal Format (UniForm), voc\u00ea pode ler tabelas Delta com clientes Iceberg e Hudi. Apache Iceberg Defini\u00e7\u00e3o e Caracter\u00edsticas Apache Iceberg \u00e9 um formato de tabela de alto desempenho para conjuntos de dados anal\u00edticos enormes. Iceberg traz a confiabilidade e simplicidade das tabelas SQL para big data, enquanto torna poss\u00edvel que motores como Spark, Trino, Flink, Presto, Hive e Impala trabalhem com seguran\u00e7a com as mesmas tabelas, ao mesmo tempo. Funcionalidades Principais SQL Expressivo : Iceberg suporta comandos SQL flex\u00edveis para mesclar novos dados, atualizar linhas existentes e realizar exclus\u00f5es direcionadas. Iceberg pode reescrever avidamente arquivos de dados para desempenho de leitura, ou pode usar deltas de exclus\u00e3o para atualiza\u00e7\u00f5es mais r\u00e1pidas. Evolu\u00e7\u00e3o Completa de Esquema : A evolu\u00e7\u00e3o de esquema funciona perfeitamente. Adicionar uma coluna n\u00e3o trar\u00e1 de volta dados antigos. Adicionar uma coluna com um valor padr\u00e3o n\u00e3o reescreve os dados existentes. Os tipos de coluna podem ser promovidos sem reescrever dados. Formato de Tabela Aberta : Iceberg \u00e9 um formato de tabela aberta para conjuntos de dados anal\u00edticos, projetado para abordar os desafios da gest\u00e3o e consulta de grandes conjuntos de dados. Benef\u00edcios C\u00f3digo Aberto : Apache Iceberg \u00e9 um projeto de c\u00f3digo aberto, o que significa que \u00e9 gratuito para usar e modificar. Alto Desempenho : Oferece uma forma r\u00e1pida e eficiente de processar grandes conjuntos de dados em escala. Compatibilidade : Funciona com v\u00e1rios motores de processamento de dados, incluindo Spark, Trino, Flink, Presto, Hive e Impala. Confiabilidade : Traz a confiabilidade e simplicidade das tabelas SQL para big data. Casos de Uso Apache Iceberg \u00e9 particularmente \u00fatil para: - Processamento de grandes conjuntos de dados anal\u00edticos - Ambientes onde m\u00faltiplos motores de processamento precisam acessar os mesmos dados - Cen\u00e1rios que exigem evolu\u00e7\u00e3o de esquema sem interrup\u00e7\u00e3o - Opera\u00e7\u00f5es que necessitam de transa\u00e7\u00f5es ACID em dados de big data E se voc\u00ea leu at\u00e9 aqui, parab\u00e9ns!","title":"Pesquisa sobre Apache Spark, Delta Lake e Apache Iceberg"},{"location":"pesquisa/#pesquisa-sobre-apache-spark-delta-lake-e-apache-iceberg","text":"","title":"Pesquisa sobre Apache Spark, Delta Lake e Apache Iceberg"},{"location":"pesquisa/#apache-spark","text":"","title":"Apache Spark"},{"location":"pesquisa/#definicao-e-caracteristicas","text":"O Apache Spark \u00e9 um sistema de processamento distribu\u00eddo de c\u00f3digo aberto usado para workloads de big data. O sistema utiliza armazenamento em cache na mem\u00f3ria e execu\u00e7\u00e3o otimizada de consultas para processar dados de qualquer tamanho de forma eficiente.","title":"Defini\u00e7\u00e3o e Caracter\u00edsticas"},{"location":"pesquisa/#historia","text":"O Apache Spark come\u00e7ou em 2009 como um projeto de pesquisa no AMPLab da UC Berkeley, uma colabora\u00e7\u00e3o envolvendo estudantes, pesquisadores e professores, focada em dom\u00ednios de aplica\u00e7\u00f5es com uso intensivo de dados. O primeiro artigo intitulado \"Spark: Cluster Computing with Working Sets\" foi publicado em junho de 2010, e o Spark era de c\u00f3digo aberto sob uma licen\u00e7a BSD. Em junho de 2013, o Spark passou para o status de incuba\u00e7\u00e3o na Apache Software Foundation (ASF) e se estabeleceu como um projeto de alto n\u00edvel da Apache em fevereiro de 2014.","title":"Hist\u00f3ria"},{"location":"pesquisa/#funcionamento","text":"O Hadoop MapReduce \u00e9 um modelo de programa\u00e7\u00e3o para processar conjuntos de big data com um algoritmo distribu\u00eddo paralelo, mas tem limita\u00e7\u00f5es devido ao processo sequencial de v\u00e1rias etapas necess\u00e1rio para executar um trabalho. O Spark foi criado para resolver essas limita\u00e7\u00f5es, processando na mem\u00f3ria, reduzindo o n\u00famero de etapas em uma tarefa e reutilizando dados em v\u00e1rias opera\u00e7\u00f5es paralelas. Com o Spark, \u00e9 necess\u00e1ria apenas uma etapa em que os dados s\u00e3o lidos na mem\u00f3ria, as opera\u00e7\u00f5es s\u00e3o executadas e os resultados s\u00e3o gravados de volta, resultando em uma execu\u00e7\u00e3o muito mais r\u00e1pida. O Spark tamb\u00e9m reutiliza dados usando um cache na mem\u00f3ria para acelerar consideravelmente os algoritmos de machine learning que chamam repetidamente uma fun\u00e7\u00e3o no mesmo conjunto de dados. A reutiliza\u00e7\u00e3o de dados \u00e9 realizada por meio da cria\u00e7\u00e3o de DataFrames, uma abstra\u00e7\u00e3o sobre o Conjunto de dados resiliente distribu\u00eddo (RDD), que \u00e9 uma cole\u00e7\u00e3o de objetos armazenados em cache na mem\u00f3ria e reutilizados em v\u00e1rias opera\u00e7\u00f5es do Spark. Isso reduz drasticamente a lat\u00eancia, fazendo com que o Spark seja v\u00e1rias vezes mais r\u00e1pido que o MapReduce, especialmente ao realizar machine learning e an\u00e1lises interativas.","title":"Funcionamento"},{"location":"pesquisa/#beneficios","text":"R\u00e1pido : Por meio do armazenamento em cache na mem\u00f3ria e execu\u00e7\u00e3o otimizada de consultas, o Spark pode oferecer consultas anal\u00edticas r\u00e1pidas de dados de qualquer tamanho. Para desenvolvedores : O Apache Spark suporta de modo nativo Java, Scala, R e Python, oferecendo v\u00e1rias linguagens para a cria\u00e7\u00e3o de aplicativos. Essas APIs facilitam as coisas para seus desenvolvedores, pois ocultam a complexidade do processamento distribu\u00eddo por tr\u00e1s de operadores simples e de alto n\u00edvel que reduzem drasticamente a quantidade de c\u00f3digo necess\u00e1ria. V\u00e1rias workloads : O Apache Spark vem com a capacidade de executar v\u00e1rias workloads, incluindo consultas interativas, an\u00e1lises em tempo real, machine learning e processamento de gr\u00e1ficos. Uma aplica\u00e7\u00e3o pode combinar v\u00e1rias workloads facilmente.","title":"Benef\u00edcios"},{"location":"pesquisa/#diferencas-entre-spark-e-hadoop","text":"Al\u00e9m das diferen\u00e7as no design do Spark e do Hadoop MapReduce, muitas organiza\u00e7\u00f5es descobriram que essas estruturas de big data s\u00e3o complementares, usando-as juntas para resolver um desafio comercial mais amplo. O Hadoop \u00e9 uma estrutura de c\u00f3digo aberto que tem o Sistema de Arquivos Distribu\u00eddo do Hadoop (HDFS) como armazenamento, o YARN como gerenciamento de recursos e a programa\u00e7\u00e3o MapReduce como mecanismo de execu\u00e7\u00e3o. Em uma implementa\u00e7\u00e3o t\u00edpica do Hadoop, diferentes mecanismos de execu\u00e7\u00e3o tamb\u00e9m s\u00e3o implantados, como Spark, Tez e Presto. O Spark \u00e9 uma estrutura de c\u00f3digo aberto focada em consultas interativas, machine learning e workloads em tempo real. N\u00e3o tem seu pr\u00f3prio sistema de armazenamento, mas executa an\u00e1lises em outros sistemas de armazenamento, como o HDFS, ou em outras lojas populares, como Amazon Redshift, Amazon S3, Couchbase, Cassandra e outras.","title":"Diferen\u00e7as entre Spark e Hadoop"},{"location":"pesquisa/#delta-lake","text":"","title":"Delta Lake"},{"location":"pesquisa/#definicao-e-caracteristicas_1","text":"Delta Lake \u00e9 um framework de armazenamento de c\u00f3digo aberto que permite a constru\u00e7\u00e3o de uma arquitetura lakehouse agn\u00f3stica de formato. \u00c9 uma camada de armazenamento otimizada que proporciona a base para as tabelas em uma inst\u00e2ncia de data lake.","title":"Defini\u00e7\u00e3o e Caracter\u00edsticas"},{"location":"pesquisa/#funcionalidades-principais","text":"Transa\u00e7\u00f5es ACID : Protege seus dados com serializabilidade, o n\u00edvel mais forte de isolamento Metadados Escal\u00e1veis : Lida com tabelas de escala petabyte com bilh\u00f5es de parti\u00e7\u00f5es e arquivos com facilidade Viagem no Tempo (Time Travel) : Acessa/reverte para vers\u00f5es anteriores de dados para auditorias, rollbacks ou reprodu\u00e7\u00e3o C\u00f3digo Aberto : Orientado pela comunidade, padr\u00f5es abertos, protocolo aberto, discuss\u00f5es abertas Batch/Streaming Unificado : Sem\u00e2ntica de ingest\u00e3o exatamente uma vez para backfill e consultas interativas Evolu\u00e7\u00e3o/Aplica\u00e7\u00e3o de Esquema : Previne que dados ruins causem corrup\u00e7\u00e3o de dados Hist\u00f3rico de Auditoria : Delta Lake registra todos os detalhes de altera\u00e7\u00f5es, fornecendo um hist\u00f3rico completo de auditoria Opera\u00e7\u00f5es DML : APIs SQL, Scala/Java e Python para mesclar, atualizar e excluir conjuntos de dados","title":"Funcionalidades Principais"},{"location":"pesquisa/#integracao-com-ecossistemas","text":"Delta Lake funciona com motores de computa\u00e7\u00e3o incluindo Spark, PrestoDB, Flink, Trino, Hive, Snowflake, Google BigQuery, Athena, Redshift, Databricks, Azure Fabric e APIs para Scala, Java, Rust e Python. Com o Delta Universal Format (UniForm), voc\u00ea pode ler tabelas Delta com clientes Iceberg e Hudi.","title":"Integra\u00e7\u00e3o com Ecossistemas"},{"location":"pesquisa/#apache-iceberg","text":"","title":"Apache Iceberg"},{"location":"pesquisa/#definicao-e-caracteristicas_2","text":"Apache Iceberg \u00e9 um formato de tabela de alto desempenho para conjuntos de dados anal\u00edticos enormes. Iceberg traz a confiabilidade e simplicidade das tabelas SQL para big data, enquanto torna poss\u00edvel que motores como Spark, Trino, Flink, Presto, Hive e Impala trabalhem com seguran\u00e7a com as mesmas tabelas, ao mesmo tempo.","title":"Defini\u00e7\u00e3o e Caracter\u00edsticas"},{"location":"pesquisa/#funcionalidades-principais_1","text":"SQL Expressivo : Iceberg suporta comandos SQL flex\u00edveis para mesclar novos dados, atualizar linhas existentes e realizar exclus\u00f5es direcionadas. Iceberg pode reescrever avidamente arquivos de dados para desempenho de leitura, ou pode usar deltas de exclus\u00e3o para atualiza\u00e7\u00f5es mais r\u00e1pidas. Evolu\u00e7\u00e3o Completa de Esquema : A evolu\u00e7\u00e3o de esquema funciona perfeitamente. Adicionar uma coluna n\u00e3o trar\u00e1 de volta dados antigos. Adicionar uma coluna com um valor padr\u00e3o n\u00e3o reescreve os dados existentes. Os tipos de coluna podem ser promovidos sem reescrever dados. Formato de Tabela Aberta : Iceberg \u00e9 um formato de tabela aberta para conjuntos de dados anal\u00edticos, projetado para abordar os desafios da gest\u00e3o e consulta de grandes conjuntos de dados.","title":"Funcionalidades Principais"},{"location":"pesquisa/#beneficios_1","text":"C\u00f3digo Aberto : Apache Iceberg \u00e9 um projeto de c\u00f3digo aberto, o que significa que \u00e9 gratuito para usar e modificar. Alto Desempenho : Oferece uma forma r\u00e1pida e eficiente de processar grandes conjuntos de dados em escala. Compatibilidade : Funciona com v\u00e1rios motores de processamento de dados, incluindo Spark, Trino, Flink, Presto, Hive e Impala. Confiabilidade : Traz a confiabilidade e simplicidade das tabelas SQL para big data.","title":"Benef\u00edcios"},{"location":"pesquisa/#casos-de-uso","text":"Apache Iceberg \u00e9 particularmente \u00fatil para: - Processamento de grandes conjuntos de dados anal\u00edticos - Ambientes onde m\u00faltiplos motores de processamento precisam acessar os mesmos dados - Cen\u00e1rios que exigem evolu\u00e7\u00e3o de esquema sem interrup\u00e7\u00e3o - Opera\u00e7\u00f5es que necessitam de transa\u00e7\u00f5es ACID em dados de big data E se voc\u00ea leu at\u00e9 aqui, parab\u00e9ns!","title":"Casos de Uso"}]}