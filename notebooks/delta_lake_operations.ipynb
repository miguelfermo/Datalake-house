{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operações com Delta Lake\n",
    "\n",
    "Este notebook demonstra as operações básicas de INSERT, UPDATE e DELETE usando Delta Lake com Apache Spark.\n",
    "\n",
    "## Configuração do Ambiente\n",
    "\n",
    "Primeiro, vamos configurar o ambiente e criar uma sessão Spark com suporte ao Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Importar as bibliotecas necessárias\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Adicionar o diretório src ao PYTHONPATH\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(os.path.join(project_dir, 'src'))\n",
    "\n",
    "# Importar os módulos do projeto\n",
    "from spark_delta_iceberg.spark_session import create_spark_session\n",
    "from spark_delta_iceberg.delta_operations import DeltaLakeOperations\n",
    "from spark_delta_iceberg.sample_data import create_sample_dataframe, create_sample_update_dataframe\n",
    "\n",
    "# Criar a sessão Spark\n",
    "spark = create_spark_session(\"DeltaLakeDemo\")\n",
    "\n",
    "# Criar uma instância da classe DeltaLakeOperations\n",
    "delta_ops = DeltaLakeOperations(spark)\n",
    "\n",
    "# Definir o caminho para a tabela Delta\n",
    "delta_table_path = os.path.join(project_dir, \"spark-warehouse\", \"vendas_delta\")\n",
    "\n",
    "print(\"Ambiente configurado com sucesso!\")\n",
    "print(f\"Versão do Spark: {spark.version}\")\n",
    "print(f\"Caminho da tabela Delta: {delta_table_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de Dados de Exemplo\n",
    "\n",
    "Vamos criar um DataFrame de exemplo com dados de vendas para usar em nossas operações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Criar um DataFrame de exemplo\n",
    "df_vendas = create_sample_dataframe(spark)\n",
    "\n",
    "# Exibir o esquema\n",
    "print(\"Esquema do DataFrame:\")\n",
    "df_vendas.printSchema()\n",
    "\n",
    "# Exibir os dados\n",
    "print(\"\\nDados do DataFrame:\")\n",
    "df_vendas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operação INSERT (Criação da Tabela Delta)\n",
    "\n",
    "Vamos criar uma tabela Delta Lake com os dados de vendas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Criar a tabela Delta com os dados de vendas\n",
    "delta_ops.create_table(\n",
    "    df=df_vendas,\n",
    "    table_path=delta_table_path,\n",
    "    mode=\"overwrite\",\n",
    "    partition_by=[\"estado\"]\n",
    ")\n",
    "\n",
    "# Ler a tabela Delta para verificar\n",
    "df_delta = delta_ops.read_table(delta_table_path)\n",
    "print(\"\\nDados da tabela Delta:\")\n",
    "df_delta.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operação INSERT (Adicionando Novos Dados)\n",
    "\n",
    "Vamos adicionar novos dados à tabela Delta existente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Criar novos dados para inserção\n",
    "novos_dados = [\n",
    "    (11, \"Produto K\", \"Eletrônicos\", 1800.00, \"2023-01-24\", \"RS\"),\n",
    "    (12, \"Produto L\", \"Alimentos\", 45.60, \"2023-01-25\", \"PR\")\n",
    "]\n",
    "\n",
    "# Criar DataFrame com os novos dados\n",
    "schema = \"id INT, nome STRING, categoria STRING, preco DOUBLE, data_venda STRING, estado STRING\"\n",
    "df_novos = spark.createDataFrame(novos_dados, schema)\n",
    "\n",
    "# Inserir os novos dados na tabela Delta\n",
    "delta_ops.insert_data(\n",
    "    table_path=delta_table_path,\n",
    "    new_data=df_novos,\n",
    "    mode=\"append\"\n",
    ")\n",
    "\n",
    "# Ler a tabela Delta para verificar\n",
    "df_delta_atualizado = delta_ops.read_table(delta_table_path)\n",
    "print(\"\\nDados da tabela Delta após inserção:\")\n",
    "df_delta_atualizado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operação UPDATE\n",
    "\n",
    "Vamos atualizar alguns dados na tabela Delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Atualizar o preço do Produto B\n",
    "delta_ops.update_data(\n",
    "    table_path=delta_table_path,\n",
    "    condition=\"nome = 'Produto B'\",\n",
    "    update_expr={\"preco\": \"99.90\"}\n",
    ")\n",
    "\n",
    "# Atualizar o nome e o preço do Produto E\n",
    "delta_ops.update_data(\n",
    "    table_path=delta_table_path,\n",
    "    condition=\"nome = 'Produto E'\",\n",
    "    update_expr={\n",
    "        \"nome\": \"'Produto E Premium'\",\n",
    "        \"preco\": \"149.90\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Ler a tabela Delta para verificar\n",
    "df_delta_atualizado = delta_ops.read_table(delta_table_path)\n",
    "print(\"\\nDados da tabela Delta após atualização:\")\n",
    "df_delta_atualizado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operação DELETE\n",
    "\n",
    "Vamos excluir alguns dados da tabela Delta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Excluir produtos da categoria Alimentos com preço menor que 30\n",
    "delta_ops.delete_data(\n",
    "    table_path=delta_table_path,\n",
    "    condition=\"categoria = 'Alimentos' AND preco < 30\"\n",
    ")\n",
    "\n",
    "# Ler a tabela Delta para verificar\n",
    "df_delta_atualizado = delta_ops.read_table(delta_table_path)\n",
    "print(\"\\nDados da tabela Delta após exclusão:\")\n",
    "df_delta_atualizado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operação MERGE\n",
    "\n",
    "Vamos realizar uma operação MERGE para atualizar e inserir dados em uma única operação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Criar um DataFrame com dados para atualização e inserção\n",
    "df_update = create_sample_update_dataframe(spark)\n",
    "print(\"\\nDados para MERGE:\")\n",
    "df_update.show()\n",
    "\n",
    "# Realizar a operação MERGE\n",
    "delta_ops.merge_data(\n",
    "    table_path=delta_table_path,\n",
    "    source_df=df_update,\n",
    "    merge_condition=\"target.id = source.id\",\n",
    "    matched_update={\n",
    "        \"nome\": \"source.nome\",\n",
    "        \"preco\": \"source.preco\"\n",
    "    },\n",
    "    not_matched_insert=True\n",
    ")\n",
    "\n",
    "# Ler a tabela Delta para verificar\n",
    "df_delta_atualizado = delta_ops.read_table(delta_table_path)\n",
    "print(\"\\nDados da tabela Delta após MERGE:\")\n",
    "df_delta_atualizado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Travel\n",
    "\n",
    "Vamos explorar o recurso de Time Travel do Delta Lake para acessar versões anteriores dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Obter o histórico da tabela\n",
    "history_df = delta_ops.get_history(delta_table_path)\n",
    "print(\"\\nHistórico da tabela Delta:\")\n",
    "history_df.select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(truncate=False)\n",
    "\n",
    "# Acessar a versão 0 (inicial) da tabela\n",
    "df_versao_0 = delta_ops.time_travel(delta_table_path, version=0)\n",
    "print(\"\\nDados da versão 0 da tabela Delta:\")\n",
    "df_versao_0.show()\n",
    "\n",
    "# Acessar a versão 1 da tabela (após a primeira inserção)\n",
    "df_versao_1 = delta_ops.time_travel(delta_table_path, version=1)\n",
    "print(\"\\nDados da versão 1 da tabela Delta:\")\n",
    "df_versao_1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Neste notebook, demonstramos as operações básicas de INSERT, UPDATE e DELETE usando Delta Lake com Apache Spark. Também exploramos recursos avançados como MERGE e Time Travel.\n",
    "\n",
    "O Delta Lake oferece várias vantagens para o gerenciamento de dados em data lakes:\n",
    "\n",
    "1. **Transações ACID**: Garantem a consistência dos dados mesmo em caso de falhas.\n",
    "2. **Time Travel**: Permite acessar versões anteriores dos dados para auditoria ou rollback.\n",
    "3. **Operações DML**: Suporte a operações como UPDATE, DELETE e MERGE, que não são nativas em formatos tradicionais de data lake.\n",
    "4. **Evolução de Esquema**: Permite alterar o esquema dos dados sem afetar os consumidores.\n",
    "5. **Unificação de Batch e Streaming**: Permite processar dados em batch e streaming de forma unificada."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
