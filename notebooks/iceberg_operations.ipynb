{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operações com Apache Iceberg\n",
    "\n",
    "Este notebook demonstra as operações básicas de INSERT, UPDATE e DELETE usando Apache Iceberg com Apache Spark.\n",
    "\n",
    "## Configuração do Ambiente\n",
    "\n",
    "Primeiro, vamos configurar o ambiente e criar uma sessão Spark com suporte ao Apache Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Importar as bibliotecas necessárias\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Adicionar o diretório src ao PYTHONPATH\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(os.path.join(project_dir, 'src'))\n",
    "\n",
    "# Importar os módulos do projeto\n",
    "from spark_delta_iceberg.spark_session import create_spark_session\n",
    "from spark_delta_iceberg.iceberg_operations import IcebergOperations\n",
    "from spark_delta_iceberg.sample_data import create_sample_dataframe, create_sample_update_dataframe\n",
    "\n",
    "# Criar a sessão Spark\n",
    "spark = create_spark_session(\"IcebergDemo\")\n",
    "\n",
    "# Criar uma instância da classe IcebergOperations\n",
    "iceberg_ops = IcebergOperations(spark)\n",
    "\n",
    "# Definir o nome da tabela Iceberg\n",
    "database = \"default\"\n",
    "table_name = \"vendas_iceberg\"\n",
    "\n",
    "print(\"Ambiente configurado com sucesso!\")\n",
    "print(f\"Versão do Spark: {spark.version}\")\n",
    "print(f\"Tabela Iceberg: {iceberg_ops.catalog}.{database}.{table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de Dados de Exemplo\n",
    "\n",
    "Vamos criar um DataFrame de exemplo com dados de vendas para usar em nossas operações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Criar um DataFrame de exemplo\n",
    "df_vendas = create_sample_dataframe(spark)\n",
    "\n",
    "# Exibir o esquema\n",
    "print(\"Esquema do DataFrame:\")\n",
    "df_vendas.printSchema()\n",
    "\n",
    "# Exibir os dados\n",
    "print(\"\\nDados do DataFrame:\")\n",
    "df_vendas.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operação INSERT (Criação da Tabela Iceberg)\n",
    "\n",
    "Vamos criar uma tabela Iceberg com os dados de vendas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Registrar o DataFrame como uma view temporária para criar a tabela\n",
    "df_vendas.createOrReplaceTempView(\"temp_view\")\n",
    "\n",
    "# Criar a tabela Iceberg com os dados de vendas\n",
    "iceberg_ops.create_table(\n",
    "    df=df_vendas,\n",
    "    table_name=table_name,\n",
    "    database=database,\n",
    "    partition_by=[\"estado\"]\n",
    ")\n",
    "\n",
    "# Ler a tabela Iceberg para verificar\n",
    "df_iceberg = iceberg_ops.read_table(table_name, database)\n",
    "print(\"\\nDados da tabela Iceberg:\")\n",
    "df_iceberg.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operação INSERT (Adicionando Novos Dados)\n",
    "\n",
    "Vamos adicionar novos dados à tabela Iceberg existente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Criar novos dados para inserção\n",
    "novos_dados = [\n",
    "    (11, \"Produto K\", \"Eletrônicos\", 1800.00, \"2023-01-24\", \"RS\"),\n",
    "    (12, \"Produto L\", \"Alimentos\", 45.60, \"2023-01-25\", \"PR\")\n",
    "]\n",
    "\n",
    "# Criar DataFrame com os novos dados\n",
    "schema = \"id INT, nome STRING, categoria STRING, preco DOUBLE, data_venda STRING, estado STRING\"\n",
    "df_novos = spark.createDataFrame(novos_dados, schema)\n",
    "\n",
    "# Inserir os novos dados na tabela Iceberg\n",
    "iceberg_ops.insert_data(\n",
    "    df=df_novos,\n",
    "    table_name=table_name,\n",
    "    database=database\n",
    ")\n",
    "\n",
    "# Ler a tabela Iceberg para verificar\n",
    "df_iceberg_atualizado = iceberg_ops.read_table(table_name, database)\n",
    "print(\"\\nDados da tabela Iceberg após inserção:\")\n",
    "df_iceberg_atualizado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operação UPDATE\n",
    "\n",
    "Vamos atualizar alguns dados na tabela Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Atualizar o preço do Produto B\n",
    "iceberg_ops.update_data(\n",
    "    table_name=table_name,\n",
    "    database=database,\n",
    "    condition=\"nome = 'Produto B'\",\n",
    "    update_expr={\"preco\": \"99.90\"}\n",
    ")\n",
    "\n",
    "# Atualizar o nome e o preço do Produto E\n",
    "iceberg_ops.update_data(\n",
    "    table_name=table_name,\n",
    "    database=database,\n",
    "    condition=\"nome = 'Produto E'\",\n",
    "    update_expr={\n",
    "        \"nome\": \"'Produto E Premium'\",\n",
    "        \"preco\": \"149.90\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Ler a tabela Iceberg para verificar\n",
    "df_iceberg_atualizado = iceberg_ops.read_table(table_name, database)\n",
    "print(\"\\nDados da tabela Iceberg após atualização:\")\n",
    "df_iceberg_atualizado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operação DELETE\n",
    "\n",
    "Vamos excluir alguns dados da tabela Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Excluir produtos da categoria Alimentos com preço menor que 30\n",
    "iceberg_ops.delete_data(\n",
    "    table_name=table_name,\n",
    "    database=database,\n",
    "    condition=\"categoria = 'Alimentos' AND preco < 30\"\n",
    ")\n",
    "\n",
    "# Ler a tabela Iceberg para verificar\n",
    "df_iceberg_atualizado = iceberg_ops.read_table(table_name, database)\n",
    "print(\"\\nDados da tabela Iceberg após exclusão:\")\n",
    "df_iceberg_atualizado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operação MERGE\n",
    "\n",
    "Vamos realizar uma operação MERGE para atualizar e inserir dados em uma única operação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Criar um DataFrame com dados para atualização e inserção\n",
    "df_update = create_sample_update_dataframe(spark)\n",
    "print(\"\\nDados para MERGE:\")\n",
    "df_update.show()\n",
    "\n",
    "# Realizar a operação MERGE\n",
    "iceberg_ops.merge_data(\n",
    "    source_df=df_update,\n",
    "    table_name=table_name,\n",
    "    database=database,\n",
    "    merge_condition=\"target.id = source.id\",\n",
    "    matched_update={\n",
    "        \"nome\": \"source.nome\",\n",
    "        \"preco\": \"source.preco\"\n",
    "    },\n",
    "    not_matched_insert=True\n",
    ")\n",
    "\n",
    "# Ler a tabela Iceberg para verificar\n",
    "df_iceberg_atualizado = iceberg_ops.read_table(table_name, database)\n",
    "print(\"\\nDados da tabela Iceberg após MERGE:\")\n",
    "df_iceberg_atualizado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Travel\n",
    "\n",
    "Vamos explorar o recurso de Time Travel do Apache Iceberg para acessar versões anteriores dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Obter os snapshots da tabela\n",
    "snapshots_df = iceberg_ops.get_snapshots(table_name, database)\n",
    "print(\"\\nSnapshots da tabela Iceberg:\")\n",
    "snapshots_df.select(\"snapshot_id\", \"timestamp\", \"operation\").show(truncate=False)\n",
    "\n",
    "# Obter o ID do primeiro snapshot\n",
    "first_snapshot_id = snapshots_df.select(\"snapshot_id\").first()[0]\n",
    "\n",
    "# Acessar o primeiro snapshot da tabela\n",
    "df_primeiro_snapshot = iceberg_ops.time_travel(table_name, database, snapshot_id=first_snapshot_id)\n",
    "print(f\"\\nDados do primeiro snapshot (ID: {first_snapshot_id}) da tabela Iceberg:\")\n",
    "df_primeiro_snapshot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evolução de Esquema\n",
    "\n",
    "Vamos demonstrar a evolução de esquema no Apache Iceberg, adicionando uma nova coluna à tabela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Adicionar uma nova coluna à tabela\n",
    "full_table_name = f\"{iceberg_ops.catalog}.{database}.{table_name}\"\n",
    "spark.sql(f\"ALTER TABLE {full_table_name} ADD COLUMN desconto DOUBLE\")\n",
    "\n",
    "# Atualizar alguns registros com valores para a nova coluna\n",
    "iceberg_ops.update_data(\n",
    "    table_name=table_name,\n",
    "    database=database,\n",
    "    condition=\"categoria = 'Eletrônicos'\",\n",
    "    update_expr={\"desconto\": \"preco * 0.1\"}\n",
    ")\n",
    "\n",
    "# Ler a tabela Iceberg para verificar\n",
    "df_iceberg_atualizado = iceberg_ops.read_table(table_name, database)\n",
    "print(\"\\nDados da tabela Iceberg após evolução de esquema:\")\n",
    "df_iceberg_atualizado.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Neste notebook, demonstramos as operações básicas de INSERT, UPDATE e DELETE usando Apache Iceberg com Apache Spark. Também exploramos recursos avançados como MERGE, Time Travel e Evolução de Esquema.\n",
    "\n",
    "O Apache Iceberg oferece várias vantagens para o gerenciamento de dados em data lakes:\n",
    "\n",
    "1. **Formato de Tabela Aberta**: Iceberg é um formato de tabela aberto que permite que diferentes motores de processamento acessem os mesmos dados.\n",
    "2. **Transações ACID**: Garantem a consistência dos dados mesmo em caso de falhas.\n",
    "3. **Time Travel**: Permite acessar versões anteriores dos dados para auditoria ou rollback.\n",
    "4. **Evolução de Esquema**: Permite alterar o esquema dos dados sem afetar os consumidores.\n",
    "5. **Operações SQL Expressivas**: Suporte a operações como UPDATE, DELETE e MERGE, que não são nativas em formatos tradicionais de data lake.\n",
    "6. **Particionamento Oculto**: O particionamento é gerenciado pelo Iceberg, não pelo usuário, o que simplifica o uso."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
