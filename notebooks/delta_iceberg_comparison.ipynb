{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparação entre Delta Lake e Apache Iceberg\n",
    "\n",
    "Este notebook apresenta uma comparação entre Delta Lake e Apache Iceberg, destacando suas semelhanças, diferenças e casos de uso específicos.\n",
    "\n",
    "## Configuração do Ambiente\n",
    "\n",
    "Primeiro, vamos configurar o ambiente e criar uma sessão Spark com suporte a ambas as tecnologias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar as bibliotecas necessárias\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Adicionar o diretório src ao PYTHONPATH\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(os.path.join(project_dir, 'src'))\n",
    "\n",
    "# Importar os módulos do projeto\n",
    "from spark_delta_iceberg.spark_session import create_spark_session\n",
    "from spark_delta_iceberg.delta_operations import DeltaLakeOperations\n",
    "from spark_delta_iceberg.iceberg_operations import IcebergOperations\n",
    "from spark_delta_iceberg.sample_data import load_public_dataset\n",
    "\n",
    "# Criar a sessão Spark\n",
    "spark = create_spark_session(\"DeltaIcebergComparison\")\n",
    "\n",
    "# Criar instâncias das classes de operações\n",
    "delta_ops = DeltaLakeOperations(spark)\n",
    "iceberg_ops = IcebergOperations(spark)\n",
    "\n",
    "print(\"Ambiente configurado com sucesso!\")\n",
    "print(f\"Versão do Spark: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento de Dados Públicos\n",
    "\n",
    "Vamos carregar um conjunto de dados públicos para usar em nossa comparação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar o dataset público\n",
    "df_vendas, dataset_description = load_public_dataset(spark)\n",
    "\n",
    "# Exibir a descrição do dataset\n",
    "# print(dataset_description)\n",
    "\n",
    "# Exibir o esquema\n",
    "print(\"\\nEsquema do DataFrame:\")\n",
    "df_vendas.printSchema()\n",
    "\n",
    "# Exibir uma amostra dos dados\n",
    "print(\"\\nAmostra dos dados:\")\n",
    "df_vendas.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo ER e Estrutura de Dados\n",
    "\n",
    "Vamos definir o modelo ER para nossos dados de vendas de supermercado.\n",
    "\n",
    "### Modelo ER\n",
    "\n",
    "```\n",
    "+----------------+        +----------------+        +----------------+\n",
    "|    Cliente     |        |     Venda      |        |    Produto     |\n",
    "+----------------+        +----------------+        +----------------+\n",
    "| id_cliente (PK)|<-----> | id_venda (PK)  | <----->| id_produto (PK)|\n",
    "| nome           |        | data           |        | nome           |\n",
    "| genero         |        | id_cliente (FK)|        | categoria      |\n",
    "| tipo_cliente   |        | id_produto (FK)|        | preco_unitario |\n",
    "| cidade         |        | quantidade     |        | custo          |\n",
    "+----------------+        | preco_total    |        +----------------+\n",
    "                          | pagamento      |\n",
    "                          | avaliacao      |\n",
    "                          +----------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir os caminhos e nomes das tabelas\n",
    "delta_path = os.path.join(project_dir, \"spark-warehouse\", \"vendas_supermercado_delta\")\n",
    "iceberg_db = \"default\"\n",
    "iceberg_table = \"vendas_supermercado_iceberg\"\n",
    "\n",
    "# Criar a tabela Delta Lake\n",
    "delta_ops.create_table(\n",
    "    df=df_vendas,\n",
    "    table_path=delta_path,\n",
    "    mode=\"overwrite\",\n",
    "    partition_by=[\"Payment\"]\n",
    ")\n",
    "\n",
    "# Criar a tabela Iceberg\n",
    "df_vendas.createOrReplaceTempView(\"temp_view\")\n",
    "iceberg_ops.create_table(\n",
    "    df=df_vendas,\n",
    "    table_name=iceberg_table,\n",
    "    database=iceberg_db,\n",
    "    partition_by=[\"Payment\"]\n",
    ")\n",
    "\n",
    "print(\"Tabelas criadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação de Desempenho: INSERT\n",
    "\n",
    "Vamos comparar o desempenho das operações INSERT entre Delta Lake e Apache Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Criar novos dados para inserção\n",
    "# Vamos duplicar os dados existentes e modificar alguns valores\n",
    "df_novos = df_vendas.limit(100).withColumn(\"Invoice ID\", lit(\"New-\") + col(\"Invoice ID\"))\n",
    "\n",
    "# Medir o tempo para inserção no Delta Lake\n",
    "start_time = time.time()\n",
    "delta_ops.insert_data(\n",
    "    table_path=delta_path,\n",
    "    new_data=df_novos,\n",
    "    mode=\"append\"\n",
    ")\n",
    "delta_insert_time = time.time() - start_time\n",
    "print(f\"Tempo de inserção no Delta Lake: {delta_insert_time:.2f} segundos\")\n",
    "\n",
    "# Medir o tempo para inserção no Iceberg\n",
    "start_time = time.time()\n",
    "iceberg_ops.insert_data(\n",
    "    df=df_novos,\n",
    "    table_name=iceberg_table,\n",
    "    database=iceberg_db\n",
    ")\n",
    "iceberg_insert_time = time.time() - start_time\n",
    "print(f\"Tempo de inserção no Apache Iceberg: {iceberg_insert_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação de Desempenho: UPDATE\n",
    "\n",
    "Vamos comparar o desempenho das operações UPDATE entre Delta Lake e Apache Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir o tempo para atualização no Delta Lake\n",
    "start_time = time.time()\n",
    "delta_ops.update_data(\n",
    "    table_path=delta_path,\n",
    "    condition=\"`Product line` = 'Health and beauty'\",\n",
    "    update_expr={\"Rating\": \"Rating + 0.5\"}\n",
    ")\n",
    "delta_update_time = time.time() - start_time\n",
    "print(f\"Tempo de atualização no Delta Lake: {delta_update_time:.2f} segundos\")\n",
    "\n",
    "# Medir o tempo para atualização no Iceberg\n",
    "start_time = time.time()\n",
    "iceberg_ops.update_data(\n",
    "    table_name=iceberg_table,\n",
    "    database=iceberg_db,\n",
    "    condition=\"`Product line` = 'Health and beauty'\",\n",
    "    update_expr={\"Rating\": \"Rating + 0.5\"}\n",
    ")\n",
    "iceberg_update_time = time.time() - start_time\n",
    "print(f\"Tempo de atualização no Apache Iceberg: {iceberg_update_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação de Desempenho: DELETE\n",
    "\n",
    "Vamos comparar o desempenho das operações DELETE entre Delta Lake e Apache Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Medir o tempo para exclusão no Delta Lake\n",
    "start_time = time.time()\n",
    "delta_ops.delete_data(\n",
    "    table_path=delta_path,\n",
    "    condition=\"`Product line` = 'Electronic accessories' AND Quantity < 3\"\n",
    ")\n",
    "delta_delete_time = time.time() - start_time\n",
    "print(f\"Tempo de exclusão no Delta Lake: {delta_delete_time:.2f} segundos\")\n",
    "\n",
    "# Medir o tempo para exclusão no Iceberg\n",
    "start_time = time.time()\n",
    "iceberg_ops.delete_data(\n",
    "    table_name=iceberg_table,\n",
    "    database=iceberg_db,\n",
    "    condition=\"`Product line` = 'Electronic accessories' AND Quantity < 3\"\n",
    ")\n",
    "iceberg_delete_time = time.time() - start_time\n",
    "print(f\"Tempo de exclusão no Apache Iceberg: {iceberg_delete_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resumo da Comparação\n",
    "\n",
    "Vamos resumir os resultados da comparação entre Delta Lake e Apache Iceberg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Dados para o gráfico\n",
    "operations = ['INSERT', 'UPDATE', 'DELETE']\n",
    "delta_times = [delta_insert_time, delta_update_time, delta_delete_time]\n",
    "iceberg_times = [iceberg_insert_time, iceberg_update_time, iceberg_delete_time]\n",
    "\n",
    "# Criar o gráfico de barras\n",
    "x = np.arange(len(operations))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "rects1 = ax.bar(x - width/2, delta_times, width, label='Delta Lake')\n",
    "rects2 = ax.bar(x + width/2, iceberg_times, width, label='Apache Iceberg')\n",
    "\n",
    "# Adicionar rótulos e título\n",
    "ax.set_ylabel('Tempo (segundos)')\n",
    "ax.set_title('Comparação de Desempenho: Delta Lake vs Apache Iceberg')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(operations)\n",
    "ax.legend()\n",
    "\n",
    "# Adicionar valores nas barras\n",
    "def autolabel(rects):\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate(f'{height:.2f}',\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparação de Recursos\n",
    "\n",
    "### Semelhanças entre Delta Lake e Apache Iceberg\n",
    "\n",
    "1. **Transações ACID**: Ambos oferecem suporte a transações ACID para garantir a consistência dos dados.\n",
    "2. **Time Travel**: Ambos permitem acessar versões anteriores dos dados.\n",
    "3. **Operações DML**: Ambos suportam operações como INSERT, UPDATE, DELETE e MERGE.\n",
    "4. **Evolução de Esquema**: Ambos permitem alterar o esquema dos dados sem afetar os consumidores.\n",
    "5. **Formato de Arquivo Base**: Ambos usam o formato Parquet como formato de arquivo base.\n",
    "\n",
    "### Diferenças entre Delta Lake e Apache Iceberg\n",
    "\n",
    "| Característica | Delta Lake | Apache Iceberg |\n",
    "|---------------|------------|----------------|\n",
    "| **Origem** | Desenvolvido pela Databricks | Projeto da Apache Software Foundation |\n",
    "| **Metadados** | Armazenados em arquivos JSON | Armazenados em arquivos Avro |\n",
    "| **Integração** | Melhor integração com Databricks | Melhor integração com ecossistema Apache |\n",
    "| **Particionamento** | Particionamento explícito | Particionamento oculto |\n",
    "| **Compactação** | Suporte nativo a compactação de arquivos pequenos | Requer configuração adicional |\n",
    "| **Streaming** | Suporte nativo a streaming com Structured Streaming | Suporte limitado a streaming |\n",
    "| **Comunidade** | Comunidade menor, mais focada em Databricks | Comunidade maior e mais diversificada |\n",
    "\n",
    "### Casos de Uso Recomendados\n",
    "\n",
    "**Delta Lake é mais adequado para:**\n",
    "- Ambientes Databricks\n",
    "- Casos de uso com streaming em tempo real\n",
    "- Quando a compactação automática de arquivos pequenos é importante\n",
    "- Quando a integração com o ecossistema Databricks é necessária\n",
    "\n",
    "**Apache Iceberg é mais adequado para:**\n",
    "- Ambientes multi-engine (Spark, Flink, Presto, etc.)\n",
    "- Quando a independência de fornecedor é importante\n",
    "- Quando o particionamento oculto é desejado\n",
    "- Quando a integração com o ecossistema Apache é necessária"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusão\n",
    "\n",
    "Neste notebook, comparamos o Delta Lake e o Apache Iceberg em termos de desempenho e recursos. Ambas as tecnologias oferecem recursos semelhantes para gerenciamento de dados em data lakes, como transações ACID, time travel e operações DML.\n",
    "\n",
    "A escolha entre Delta Lake e Apache Iceberg depende principalmente do ecossistema em que você está trabalhando e dos requisitos específicos do seu caso de uso. Se você está trabalhando principalmente com Databricks, o Delta Lake pode ser a escolha mais natural. Se você precisa de interoperabilidade entre diferentes motores de processamento, o Apache Iceberg pode ser mais adequado.\n",
    "\n",
    "Em termos de desempenho, ambas as tecnologias são comparáveis, com pequenas diferenças dependendo da operação específica e do tamanho dos dados. É recomendável realizar testes de desempenho com seus próprios dados e casos de uso para determinar qual tecnologia é mais adequada para suas necessidades. Caso você tenha lido até aqui, parabéns!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
